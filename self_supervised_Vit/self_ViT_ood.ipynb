{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1941f8-6c8e-4c7f-87ed-a5e4b0ce3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1693da48-4742-456a-a388-e91f2bf07052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3265111f-f46b-409c-b361-43a6bcca25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./runs/2_32_100_1e-3_1e-10_Adam_0_25_3class/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f4fa41-a709-45d5-9d3b-487068cf734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decided_model = f\"{path}/best-model.pt\" #Changed to selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d18ae2-6920-47e1-a3e6-d089fabcac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ViTDino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86ed03a-7f11-4d95-addc-484619a49cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/bahadir/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "model = ViTDino(out_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495263cd-f064-4e27-bade-a66dc2ea9336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTDino(\n",
       "  (model): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(8, 8), stride=(8, 8))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=3, bias=True)\n",
       "    )\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(decided_model))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e8491c-f73a-42ef-b1b9-ba6a92234e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import create_splits\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef84c9d1-8b37-4b84-8131-5f8bc12608f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev train example count: 791, cur train example count: 752\n",
      "Image dir: 683, Image labels: 683\n",
      "Image dir: 35, Image labels: 35\n",
      "Image dir: 703, Image labels: 703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, val_set, test_set = create_splits(dataset_type=\"5class\")\n",
    "set(test_set.img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd653b89-7cc0-428e-98cc-98112959cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev train example count: 791, cur train example count: 752\n",
      "Image dir: 681, Image labels: 681\n",
      "Image dir: 31, Image labels: 31\n",
      "Image dir: 703, Image labels: 703\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = create_splits(dataset_type=\"5class\")\n",
    "\n",
    "#Conversion Part\n",
    "conversion = lambda x: 1 if ((x>=1) & (x<4)) else 2 if (x==4) else 0\n",
    "test_set.img_labels = [conversion(i) for i in test_set.img_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4fd26f-f648-4f33-b761-5551e38b2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2650eb16-9d5f-4159-a126-64dd700411c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(test_set.img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac044b2-cbd4-4257-8914-4b8e533eb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, model, criterion, test_loader, savedir):\n",
    "    cur_test_loss = 0\n",
    "    test_tot, test_corr = 0, 0\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels, all_indices = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, index in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outs = model(images)\n",
    "            test_loss = criterion(outs, labels)\n",
    "            cur_test_loss += test_loss.item()\n",
    "\n",
    "            _, preds = torch.max(outs.data, 1)\n",
    "            test_tot += labels.size(0)\n",
    "            test_corr += (preds == labels).sum().item()\n",
    "\n",
    "            preds = preds.cpu().tolist()\n",
    "            labels = labels.cpu().tolist()\n",
    "            index = index.tolist()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "            all_indices.extend(index)\n",
    "            \n",
    "    test_loss = cur_test_loss / len(test_loader)\n",
    "    test_acc = test_corr / test_tot\n",
    "\n",
    "    print(f'Test loss: {test_loss:.3f}, Test accuracy: {test_acc:.3f}')\n",
    "\n",
    "    #Print predictions by class\n",
    "    confusion_matrix = pd.crosstab(np.array(all_preds), np.array(all_labels), rownames=[\"Predictions\"], colnames=[\"Actuals\"]) \n",
    "    print(confusion_matrix)\n",
    "\n",
    "    #Accuracy Results\n",
    "    classification_res = classification_report(all_labels, all_preds, digits=3)\n",
    "    print(classification_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1507efbd-de45-4a96-b1d6-d89fa4d632b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.100, Test accuracy: 0.977\n",
      "Actuals        0    1    2\n",
      "Predictions               \n",
      "0            177    0    0\n",
      "1              0  409    4\n",
      "2              1   11  101\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.994     0.997       178\n",
      "           1      0.990     0.974     0.982       420\n",
      "           2      0.894     0.962     0.927       105\n",
      "\n",
      "    accuracy                          0.977       703\n",
      "   macro avg      0.961     0.977     0.969       703\n",
      "weighted avg      0.978     0.977     0.978       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(device, model, nn.CrossEntropyLoss(), test_loader, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4d3dd-f83e-4116-9e17-f7e8b6089c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
